# -*- coding: utf-8 -*-
"""AutoEncoders Based Recommender System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KDhXIkO6rEGxaxOMqTMExOub9EXLTKj3
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import torch
import pandas as pd
import torch.nn as nn
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
from torch.autograd import Variable

!unzip -uq "/content/drive/My Drive/Colab Notebooks/Boltzmann Machine/Boltzmann_Machines.zip" -d "/content/drive/My Drive/Colab Notebooks/AutoEncoders based recommendation system"

# importing datasets

movies=pd.read_csv('/content/drive/My Drive/Colab Notebooks/Boltzmann Machine/Boltzmann_Machines/ml-1m/movies.dat',sep='::',header=None, engine='python',encoding='latin-1')
users=pd.read_csv('/content/drive/My Drive/Colab Notebooks/Boltzmann Machine/Boltzmann_Machines/ml-1m/users.dat',sep='::',header=None, engine='python',encoding='latin-1')
ratings=pd.read_csv('/content/drive/My Drive/Colab Notebooks/Boltzmann Machine/Boltzmann_Machines/ml-1m/ratings.dat',sep='::',header=None, engine='python',encoding='latin-1')

movies

users

ratings

train_set=pd.read_csv('/content/drive/My Drive/Colab Notebooks/Boltzmann Machine/Boltzmann_Machines/ml-100k/u1.base', delimiter='\t')
train_set=np.array(train_set, dtype='int')
test_set=pd.read_csv('/content/drive/My Drive/Colab Notebooks/Boltzmann Machine/Boltzmann_Machines/ml-100k/u1.test', delimiter='\t')
test_set=np.array(test_set,dtype='int')

train_set

test_set

nb_users=int(max(max(train_set[:,0]),max(test_set[:,0])))
nb_movies=int(max(max(train_set[:,1]),max(test_set[:,1])))

nb_users

nb_movies

def convert(data):
  new_data=[]
  for id_users in range(1,nb_users+1):
    id_movies=data[:,1][data[:,0]==id_users]
    id_ratings=data[:,2][data[:,0]==id_users]
    ratings=np.zeros(nb_movies)
    ratings[id_movies-1]=id_ratings
    new_data.append(list(ratings))
  return new_data

train_set=convert(train_set)

test_set=convert(test_set)

np.shape(train_set)

np.shape(test_set)

train_set=torch.FloatTensor(train_set)
test_set=torch.FloatTensor(test_set)

train_set

test_set

# neural network architecture creation

class StackAutoEncoder(nn.Module):
  def __init__(self,):
    super(StackAutoEncoder, self).__init__()
    self.fullconn1=nn.Linear(nb_movies,20)  #start full connection, nb_movies: input neurons, 20: hidden layer and output of nb_movies
    self.fullconn2=nn.Linear(20,10)
    self.fullconn3=nn.Linear(10,20)
    self.fullconn4=nn.Linear(20,nb_movies)  
    self.activation=nn.Sigmoid()

  def forward(self,x):
    x=self.activation(self.fullconn1(x))
    x=self.activation(self.fullconn2(x))
    x=self.activation(self.fullconn3(x))
    x=(self.fullconn4(x))
    return x
sae=StackAutoEncoder()
loss_fun=nn.MSELoss()  #defining the loss function(mean squared error)
optimizer=optim.RMSprop(sae.parameters(), lr=0.01, weight_decay=0.5)

# training the model

nb_epoch=250
for i in range (1,nb_epoch+1):
  train_loss=0
  s=0.
  for id_users in range(nb_users):
    input=Variable(train_set[id_users]).unsqueeze(0)  # adding additional dimension at index pos 0 after the train_set
    target=input.clone()  # input and target dims are same
    if torch.sum(target.data>0)>0 : # atleast one movie has been rated
       output=sae(input)
       target.require_grad=False # no optimizers are used at the target end
       output[target==0]=0
       loss=loss_fun(target, output)
       mean_corrector=nb_movies/float(torch.sum(target.data>0)+1e-10)
       loss.backward()  # back propagation
       train_loss+=np.sqrt(loss.data*mean_corrector)
       s+=1.
       optimizer.step()  
    
  print("Epoch: "+str(i)+ " Loss: "+str(train_loss/s))

test_loss=0 # testing model
s=0.
for id_users in range(nb_users):
    input=Variable(train_set[id_users]).unsqueeze(0)  # adding additional dimension at index pos 0 after the train_set
    target=input.clone()  # input and target dims are same
    if torch.sum(target.data>0)>0 : # atleast one movie has been rated
       output=sae(input)
       target.require_grad=False # no optimizers are used at the target end
       output[target==0]=0
       loss=loss_fun(target, output)
       mean_corrector=nb_movies/float(torch.sum(target.data>0)+1e-10)
       test_loss+=np.sqrt(loss.data*mean_corrector)
       s+=1.
       optimizer.step()  
    
print("Test_Loss: "+str(test_loss/s))

